{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70db077d",
   "metadata": {},
   "source": [
    "**Author:** Boris Kundu\n",
    "\n",
    "**Problem Statement**: Comparison of different adaptive learning optimizers.\n",
    "\n",
    "**Dataset** Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79c0607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7234c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64a6d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input parameters\n",
    "n1 = len(iris.feature_names)  # input size\n",
    "k = len(iris.target_names)    # output size\n",
    "n2 = 5                        # hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7db9181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize weights and biases\n",
    "W1 = torch.randn(n1, n2, dtype=torch.double, requires_grad=True)\n",
    "b1 = torch.randn(n2, dtype=torch.double, requires_grad=True)\n",
    "W2 = torch.randn(n2, k, dtype=torch.double, requires_grad=True)\n",
    "b2 = torch.randn(k, dtype=torch.double, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66039a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input and output features\n",
    "X = torch.tensor(iris[\"data\"])\n",
    "target = torch.tensor(iris[\"target\"], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c522a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Accumulated Squared Gradient - AdaGrad\n",
    "rW1 = torch.zeros_like(W1)\n",
    "rb1 = torch.zeros_like(b1)\n",
    "rW2 = torch.zeros_like(W2)\n",
    "rb2 = torch.zeros_like(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "236e85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define system parameters\n",
    "alpha = 0.9 #Momentum\n",
    "eta = 0.01 #Learning rate\n",
    "epochs = 1000 #Iterations\n",
    "delta = 1e-7 #To avoid divide by zero in case gradient is 0\n",
    "rho = 0.9 #For RMSprop\n",
    "rho1 = 0.9 #For Adam\n",
    "rho2 = 0.999 #For Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3bb9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of initial weights for AdaGrad.\n",
    "adaW1 = W1\n",
    "adab1 = b1\n",
    "adaW2 = W2\n",
    "adab2 = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2e25ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.4532194599800896 at Epoch:0\n",
      "Loss:0.9694441040324958 at Epoch:100\n",
      "Loss:0.7634343085339962 at Epoch:200\n",
      "Loss:0.675112804981835 at Epoch:300\n",
      "Loss:0.6321122797349757 at Epoch:400\n",
      "Loss:0.603050860637645 at Epoch:500\n",
      "Loss:0.5805252328733098 at Epoch:600\n",
      "Loss:0.5618992145257825 at Epoch:700\n",
      "Loss:0.5459118800821069 at Epoch:800\n",
      "Loss:0.5318506364220132 at Epoch:900\n"
     ]
    }
   ],
   "source": [
    "#Train using AdaGrad\n",
    "for i in range(epochs):\n",
    "    o1 = X.matmul(adaW1) + adab1\n",
    "    h = o1.sigmoid()\n",
    "    o2 = h.matmul(adaW2) + adab2\n",
    "    L = F.cross_entropy(o2, target)\n",
    "    if (i%100 == 0):\n",
    "        print(f'Loss:{L.item()} at Epoch:{i}')\n",
    "    adaW1.grad = None\n",
    "    adab1.grad = None\n",
    "    adaW2.grad = None\n",
    "    adab2.grad = None\n",
    "    L.backward()\n",
    "    rb2 = rb2 + adab2.grad.square()\n",
    "    adab2.data = adab2.data - eta * adab2.grad.div(delta + rb2.sqrt())\n",
    "    rW2 = rW2 + adaW2.grad.square()\n",
    "    adaW2.data = adaW2.data - eta * adaW2.grad.div(delta + rW2.sqrt())\n",
    "    rb1 = rb1 + adab1.grad.square()\n",
    "    adab1.data = adab1.data - eta * adab1.grad.div(delta + rb1.sqrt())\n",
    "    rW1 = rW1 + adaW1.grad.square()\n",
    "    adaW1.data = adaW1.data - eta * adaW1.grad.div(delta + rW1.sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7545b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict\n",
    "def predict(features,target_class,wt1,bs1,wt2,bs2,opt):\n",
    "    o1 = features.matmul(wt1) + bs1\n",
    "    h = o1.sigmoid()\n",
    "    o2 = h.matmul(wt2) + bs2\n",
    "    ypred = o2.argmax(axis=1)\n",
    "    print(f'Predictions using {opt} is:\\n{ypred}')\n",
    "    matches = torch.eq(ypred, target).int().sum()\n",
    "    print(f'Matches using {opt} is:{matches.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4045e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using AdaGrad is:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2,\n",
      "        2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n",
      "Matches using AdaGrad is:113\n"
     ]
    }
   ],
   "source": [
    "#Predict using AdaGrad\n",
    "predict(X,target,adaW1,adab1,adaW2,adab2,'AdaGrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63e97ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of initial weights for RMSProp.\n",
    "rmsW1 = W1\n",
    "rmsb1 = b1\n",
    "rmsW2 = W2\n",
    "rmsb2 = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c863a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Accumulated Squared Gradient - RMSProp\n",
    "rW1 = torch.zeros_like(W1)\n",
    "rb1 = torch.zeros_like(b1)\n",
    "rW2 = torch.zeros_like(W2)\n",
    "rb2 = torch.zeros_like(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1424ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.519269887816031 at Epoch:0\n",
      "Loss:0.19723521340853803 at Epoch:100\n",
      "Loss:0.10321460260039333 at Epoch:200\n",
      "Loss:0.07032723341672055 at Epoch:300\n",
      "Loss:0.05336319054557142 at Epoch:400\n",
      "Loss:0.04899767935117585 at Epoch:500\n",
      "Loss:0.046967942103228875 at Epoch:600\n",
      "Loss:0.04578384541635465 at Epoch:700\n",
      "Loss:0.044984337305388325 at Epoch:800\n",
      "Loss:0.04438882637883274 at Epoch:900\n"
     ]
    }
   ],
   "source": [
    "#Train using RMSProp\n",
    "for i in range(epochs):\n",
    "    o1 = X.matmul(rmsW1) + rmsb1\n",
    "    h = o1.sigmoid()\n",
    "    o2 = h.matmul(rmsW2) + rmsb2\n",
    "    L = F.cross_entropy(o2, target)\n",
    "    if (i%100 == 0):\n",
    "        print(f'Loss:{L.item()} at Epoch:{i}')\n",
    "    rmsW1.grad = None\n",
    "    rmsb1.grad = None\n",
    "    rmsW2.grad = None\n",
    "    rmsb2.grad = None\n",
    "    L.backward()\n",
    "    rb2 = rho * rb2 + (1 - rho) * rmsb2.grad.square()\n",
    "    rmsb2.data = rmsb2.data - eta * rmsb2.grad.div(delta + rb2.sqrt())\n",
    "    rW2 = rho * rW2 + (1 - rho) * rmsW2.grad.square()\n",
    "    rmsW2.data = rmsW2.data - eta * rmsW2.grad.div(delta + rW2.sqrt())\n",
    "    rb1 = rho * rb1 + (1 - rho) * rmsb1.grad.square()\n",
    "    rmsb1.data = rmsb1.data - eta * rmsb1.grad.div(delta + rb1.sqrt())\n",
    "    rW1 = rho * rW1 + (1 - rho) * rmsW1.grad.square()\n",
    "    rmsW1.data = rmsW1.data - eta * rmsW1.grad.div(delta + rW1.sqrt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dff3d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using RMSProp is:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n",
      "Matches using RMSProp is:148\n"
     ]
    }
   ],
   "source": [
    "#Predict using RMSProp\n",
    "predict(X,target,rmsW1,rmsb1,rmsW2,rmsb2,'RMSProp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4982b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Nesterov\n",
    "vW1 = torch.zeros_like(W1)\n",
    "vb1 = torch.zeros_like(b1)\n",
    "vW2 = torch.zeros_like(W2)\n",
    "vb2 = torch.zeros_like(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a733900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Accumulated Squared Gradient - RMSProp Nesterov\n",
    "rW1 = torch.zeros_like(W1)\n",
    "rb1 = torch.zeros_like(b1)\n",
    "rW2 = torch.zeros_like(W2)\n",
    "rb2 = torch.zeros_like(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9d805fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of initial weights for RMSProp with Nesterov.\n",
    "rmsnW1 = W1\n",
    "rmsnb1 = b1\n",
    "rmsnW2 = W2\n",
    "rmsnb2 = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80d51d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.04391518756187686 at Epoch:0\n",
      "Loss:0.04204065220747992 at Epoch:100\n",
      "Loss:0.04304365578020279 at Epoch:200\n",
      "Loss:0.042231424007413546 at Epoch:300\n",
      "Loss:0.04157183575045348 at Epoch:400\n",
      "Loss:0.040993215778672674 at Epoch:500\n",
      "Loss:0.040457779671888044 at Epoch:600\n",
      "Loss:0.039944235736047856 at Epoch:700\n",
      "Loss:0.03943958616417627 at Epoch:800\n",
      "Loss:0.038934878128612446 at Epoch:900\n"
     ]
    }
   ],
   "source": [
    "#Train using RMSProp with Nesterov\n",
    "for i in range(epochs):\n",
    "    tW1 = rmsnW1.data\n",
    "    tb1 = rmsnb1.data\n",
    "    tW2 = rmsnW2.data\n",
    "    tb2 = rmsnb2.data\n",
    "    rmsnW1.data = rmsnW1.data + alpha * vW1\n",
    "    rmsnb1.data = rmsnb1.data + alpha * vb1\n",
    "    rmsnW2.data = rmsnW2.data + alpha * vW2\n",
    "    rmsnb2.data = rmsnb2.data + alpha * vb2\n",
    "    o1 = X.matmul(rmsnW1) + rmsnb1\n",
    "    h = o1.sigmoid()\n",
    "    o2 = h.matmul(rmsnW2) + rmsnb2\n",
    "    L = F.cross_entropy(o2, target)\n",
    "    if (i%100 == 0):\n",
    "        print(f'Loss:{L.item()} at Epoch:{i}')\n",
    "    rmsnW1.grad = None\n",
    "    rmsnb1.grad = None\n",
    "    rmsnW2.grad = None\n",
    "    rmsnb2.grad = None\n",
    "    L.backward()\n",
    "    rb2 = rho * rb2 + (1 - rho) * rmsnb2.grad.square()\n",
    "    vb2 = alpha * vb2 - eta * rmsnb2.grad.div(rb2.sqrt())\n",
    "    rmsnb2.data = tb2 + vb2\n",
    "    rW2 = rho * rW2 + (1 - rho) * rmsnW2.grad.square()\n",
    "    vW2 = alpha * vW2 - eta * rmsnW2.grad.div(rW2.sqrt())\n",
    "    rmsnW2.data = tW2 + vW2\n",
    "    rb1 = rho * rb1 + (1 - rho) * rmsnb1.grad.square()\n",
    "    vb1 = alpha * vb1 - eta * rmsnb1.grad.div(rb1.sqrt())\n",
    "    rmsnb1.data = tb1 + vb1\n",
    "    rW1 = rho * rW1 + (1 - rho) * rmsnW1.grad.square()\n",
    "    vW1 = alpha * vW1 - eta * rmsnW1.grad.div(rW1.sqrt())\n",
    "    rmsnW1.data = tW1 + vW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed04b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using RMSProp with Nesterov is:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n",
      "Matches using RMSProp with Nesterov is:149\n"
     ]
    }
   ],
   "source": [
    "#Predict using RMSProp with Nesterov\n",
    "predict(X,target,rmsnW1,rmsnb1,rmsnW2,rmsnb2,'RMSProp with Nesterov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80802f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize for Adam\n",
    "rW1 = torch.zeros_like(W1)\n",
    "rb1 = torch.zeros_like(b1)\n",
    "rW2 = torch.zeros_like(W2)\n",
    "rb2 = torch.zeros_like(b2)\n",
    "sW1 = torch.zeros_like(W1)\n",
    "sb1 = torch.zeros_like(b1)\n",
    "sW2 = torch.zeros_like(W2)\n",
    "sb2 = torch.zeros_like(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "724e4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make copies of initial weights for Adam\n",
    "admW1 = W1\n",
    "admb1 = b1\n",
    "admW2 = W2\n",
    "admb2 = b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a228394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0361660668124977 at Epoch:0\n",
      "Loss:0.03553527912940638 at Epoch:100\n",
      "Loss:0.0352079712652821 at Epoch:200\n",
      "Loss:0.03477218096981501 at Epoch:300\n",
      "Loss:0.03421487848021305 at Epoch:400\n",
      "Loss:0.033533424141350604 at Epoch:500\n",
      "Loss:0.03275584079750848 at Epoch:600\n",
      "Loss:0.031936878233067706 at Epoch:700\n",
      "Loss:0.03112675729748629 at Epoch:800\n",
      "Loss:0.030352907888884905 at Epoch:900\n"
     ]
    }
   ],
   "source": [
    "#Train Adam\n",
    "rho1t = rho1\n",
    "rho2t = rho2\n",
    "for i in range(epochs):\n",
    "    o1 = X.matmul(admW1) + admb1\n",
    "    h = o1.sigmoid()\n",
    "    o2 = h.matmul(admW2) + admb2\n",
    "    L = F.cross_entropy(o2, target)\n",
    "    if (i%100 == 0):\n",
    "        print(f'Loss:{L.item()} at Epoch:{i}')\n",
    "    admW1.grad = None\n",
    "    admb1.grad = None\n",
    "    admW2.grad = None\n",
    "    admb2.grad = None\n",
    "    L.backward()\n",
    "    sb2 = rho1 * sb2 + (1 - rho1) * admb2.grad\n",
    "    rb2 = rho2 * rb2 + (1 - rho2) * admb2.grad.square()\n",
    "    admb2.data = admb2.data - eta * sb2.div(1 - rho1t).div(delta + rb2.div(1 - rho2t).sqrt())\n",
    "    sW2 = rho1 * sW2 + (1 - rho1) * admW2.grad\n",
    "    rW2 = rho2 * rW2 + (1 - rho2) * admW2.grad.square()\n",
    "    admW2.data = admW2.data - eta * sW2.div(1 - rho1t).div(delta + rW2.div(1 - rho2t).sqrt())\n",
    "    sb1 = rho1 * sb1 + (1 - rho1) * admb1.grad\n",
    "    rb1 = rho2 * rb1 + (1 - rho2) * admb1.grad.square()\n",
    "    admb1.data = admb1.data - eta * sb1.div(1 - rho1t).div(delta + rb1.div(1 - rho2t).sqrt())\n",
    "    sW1 = rho1 * sW1 + (1 - rho1) * admW1.grad\n",
    "    rW1 = rho2 * rW1 + (1 - rho2) * admW1.grad.square()\n",
    "    admW1.data = admW1.data - eta * sW1.div(1 - rho1t).div(delta + rW1.div(1 - rho2t).sqrt())\n",
    "    rho1t = rho1t * rho1\n",
    "    rho2t = rho2t * rho2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6282fe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions using Adam is:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n",
      "Matches using Adam is:149\n"
     ]
    }
   ],
   "source": [
    "#Predict using Adam\n",
    "predict(X,target,admW1,admb1,admW2,admb2,'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc7f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
